{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7da02621-8c55-4743-9220-fc11564772d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import functools\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "import yaml\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import plotting\n",
    "import utils\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from data_loader import get_dataset, CustomDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d29b9-93e1-4fd2-a28b-28089b37e63f",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a79e3368-7491-461c-9b7d-b3b97510a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE = '../results/concept_eval/exp1_cv.csv'\n",
    "RESULTS_FILE2 = '../results/concept_eval/exp2_cv.csv'\n",
    "TRANSFER_LIN_FILE = '../results/concept_eval/exp_transfer_lin.csv'\n",
    "TRANSFER_MLP_FILE = '../results/concept_eval/exp_transfer_mlp.csv'\n",
    "PROCESSED_TRANSFER_FILE = '../results/concept_eval/prep_results_transfer.csv'\n",
    "\n",
    "PROCESSED_RESULTS_FILE = '../results/concept_eval/prep_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "43b41e6c-7de5-4a2b-b9b6-915085dd7007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>model type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>pooling</th>\n",
       "      <th>classifier</th>\n",
       "      <th>clf hidden size factor</th>\n",
       "      <th>emb size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>loss</th>\n",
       "      <th>group</th>\n",
       "      <th>Pearson R</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.218510</td>\n",
       "      <td>0.260668</td>\n",
       "      <td>0.178546</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>weighted mean</td>\n",
       "      <td>0.166886</td>\n",
       "      <td>0.224006</td>\n",
       "      <td>0.113047</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean (p &lt; 0.05)</td>\n",
       "      <td>0.329735</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.269221</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.228754</td>\n",
       "      <td>0.053363</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>black</td>\n",
       "      <td>0.676848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731941</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.765516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8199</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>weighted mean</td>\n",
       "      <td>0.765516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean (p &lt; 0.05)</td>\n",
       "      <td>0.765516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.772165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947267</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.758867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937766</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17111 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset                             model model type architecture  \\\n",
       "0           sbic                 bert-base-uncased       bert      encoder   \n",
       "1           sbic                 bert-base-uncased       bert      encoder   \n",
       "2           sbic                 bert-base-uncased       bert      encoder   \n",
       "3           sbic                 bert-base-uncased       bert      encoder   \n",
       "4           sbic                 bert-base-uncased       bert      encoder   \n",
       "...          ...                               ...        ...          ...   \n",
       "8198  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8199  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8200  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8201  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8202  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "\n",
       "     pooling classifier  clf hidden size factor  emb size optimizer  lr  \\\n",
       "0       mean     linear                    -1.0       768     Salsa NaN   \n",
       "1       mean     linear                    -1.0       768     Salsa NaN   \n",
       "2       mean     linear                    -1.0       768     Salsa NaN   \n",
       "3       mean     linear                    -1.0       768     Salsa NaN   \n",
       "4       mean     linear                    -1.0       768     Salsa NaN   \n",
       "...      ...        ...                     ...       ...       ...  ..   \n",
       "8198    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8199    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8200    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8201    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8202    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "\n",
       "                   loss            group  Pearson R    pvalue    PR-AUC  \\\n",
       "0     BCEWithLogitsLoss             mean   0.218510  0.260668  0.178546   \n",
       "1     BCEWithLogitsLoss    weighted mean   0.166886  0.224006  0.113047   \n",
       "2     BCEWithLogitsLoss  mean (p < 0.05)   0.329735  0.000078  0.269221   \n",
       "3     BCEWithLogitsLoss            white   0.130812  0.228754  0.053363   \n",
       "4     BCEWithLogitsLoss            black   0.676848  0.000000  0.731941   \n",
       "...                 ...              ...        ...       ...       ...   \n",
       "8198  BCEWithLogitsLoss             mean   0.765516  0.000000  0.942517   \n",
       "8199  BCEWithLogitsLoss    weighted mean   0.765516  0.000000  0.942517   \n",
       "8200  BCEWithLogitsLoss  mean (p < 0.05)   0.765516  0.000000  0.942517   \n",
       "8201  BCEWithLogitsLoss               aa   0.772165  0.000000  0.947267   \n",
       "8202  BCEWithLogitsLoss            white   0.758867  0.000000  0.937766   \n",
       "\n",
       "                Epochs  \n",
       "0     [18, 26, 28, 29]  \n",
       "1     [18, 26, 28, 29]  \n",
       "2     [18, 26, 28, 29]  \n",
       "3     [18, 26, 28, 29]  \n",
       "4     [18, 26, 28, 29]  \n",
       "...                ...  \n",
       "8198  [12, 10, 17, 10]  \n",
       "8199  [12, 10, 17, 10]  \n",
       "8200  [12, 10, 17, 10]  \n",
       "8201  [12, 10, 17, 10]  \n",
       "8202  [12, 10, 17, 10]  \n",
       "\n",
       "[17111 rows x 16 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RESULTS_FILE)\n",
    "df2 = pd.read_csv(RESULTS_FILE2)\n",
    "df = pd.concat([df, df2])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0194f0f7-a5aa-4980-ae03-0d428808d94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset train</th>\n",
       "      <th>dataset test</th>\n",
       "      <th>model</th>\n",
       "      <th>model type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>pooling</th>\n",
       "      <th>classifier</th>\n",
       "      <th>clf hidden size factor</th>\n",
       "      <th>emb size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>loss</th>\n",
       "      <th>group</th>\n",
       "      <th>PR-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta-v2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.922259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta-v2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.928535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta-v2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.915982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta-v2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.118734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta-v2</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.204196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>sbic</td>\n",
       "      <td>stereoset</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>male</td>\n",
       "      <td>0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>sbic</td>\n",
       "      <td>stereoset</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>female</td>\n",
       "      <td>0.425631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>sbic</td>\n",
       "      <td>stereoset</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6448</th>\n",
       "      <td>sbic</td>\n",
       "      <td>stereoset</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.261707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449</th>\n",
       "      <td>sbic</td>\n",
       "      <td>stereoset</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.017006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11607 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset train   dataset test                       model  model type  \\\n",
       "0       twitterAAE     twitterAAE  microsoft/deberta-v3-large  deberta-v2   \n",
       "1       twitterAAE     twitterAAE  microsoft/deberta-v3-large  deberta-v2   \n",
       "2       twitterAAE     twitterAAE  microsoft/deberta-v3-large  deberta-v2   \n",
       "3       twitterAAE  implicit_hate  microsoft/deberta-v3-large  deberta-v2   \n",
       "4       twitterAAE  implicit_hate  microsoft/deberta-v3-large  deberta-v2   \n",
       "...            ...            ...                         ...         ...   \n",
       "6445          sbic      stereoset     xlnet/xlnet-large-cased       xlnet   \n",
       "6446          sbic      stereoset     xlnet/xlnet-large-cased       xlnet   \n",
       "6447          sbic      stereoset     xlnet/xlnet-large-cased       xlnet   \n",
       "6448          sbic      stereoset     xlnet/xlnet-large-cased       xlnet   \n",
       "6449          sbic      stereoset     xlnet/xlnet-large-cased       xlnet   \n",
       "\n",
       "     architecture pooling classifier  clf hidden size factor  emb size  \\\n",
       "0         encoder    mean     linear                    -1.0      1024   \n",
       "1         encoder    mean     linear                    -1.0      1024   \n",
       "2         encoder    mean     linear                    -1.0      1024   \n",
       "3         encoder    mean     linear                    -1.0      1024   \n",
       "4         encoder    mean     linear                    -1.0      1024   \n",
       "...           ...     ...        ...                     ...       ...   \n",
       "6445      decoder    mean       MLP2                     1.0      1024   \n",
       "6446      decoder    mean       MLP2                     1.0      1024   \n",
       "6447      decoder    mean       MLP2                     1.0      1024   \n",
       "6448      decoder    mean       MLP2                     1.0      1024   \n",
       "6449      decoder    mean       MLP2                     1.0      1024   \n",
       "\n",
       "     optimizer  lr               loss      group    PR-AUC  \n",
       "0        Salsa NaN  BCEWithLogitsLoss       mean  0.922259  \n",
       "1        Salsa NaN  BCEWithLogitsLoss         aa  0.928535  \n",
       "2        Salsa NaN  BCEWithLogitsLoss      white  0.915982  \n",
       "3        Salsa NaN  BCEWithLogitsLoss       mean  0.118734  \n",
       "4        Salsa NaN  BCEWithLogitsLoss      white  0.204196  \n",
       "...        ...  ..                ...        ...       ...  \n",
       "6445     Salsa NaN  BCEWithLogitsLoss       male  0.275098  \n",
       "6446     Salsa NaN  BCEWithLogitsLoss     female  0.425631  \n",
       "6447     Salsa NaN  BCEWithLogitsLoss  christian  0.130200  \n",
       "6448     Salsa NaN  BCEWithLogitsLoss     muslim  0.261707  \n",
       "6449     Salsa NaN  BCEWithLogitsLoss     jewish  0.017006  \n",
       "\n",
       "[11607 rows x 14 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transfer = pd.read_csv(TRANSFER_LIN_FILE)\n",
    "df_transfer2 = pd.read_csv(TRANSFER_MLP_FILE)\n",
    "df_transfer = pd.concat([df_transfer, df_transfer2])\n",
    "df_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de73f06-e633-4cb0-bba2-e475c8865634",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5b9eb2ec-4f06-47cd-ba7a-d673f2733371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(results, sel_cols, target_col):\n",
    "    grouped_res = results.groupby(sel_cols, as_index=False)[target_col].mean().reset_index()\n",
    "    grouped_res[target_col+' var'] = results.groupby(sel_cols, as_index=False)[target_col].var()[target_col]\n",
    "    return grouped_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8833b9-acdb-4da3-854d-242d6bb3a2cd",
   "metadata": {},
   "source": [
    "## Preprocess results\n",
    "\n",
    "#### 1) Add protected attribute and standardized group labels + replace nans in pooling column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cd80413e-4970-467d-9ffc-62175bdd60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[pd.isna(df['pooling']),'pooling'] = 'unknown'\n",
    "df_transfer.loc[pd.isna(df_transfer['pooling']),'pooling'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "166df6ef-e98c-4ca0-9a2f-08b05c2df386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>model type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>pooling</th>\n",
       "      <th>classifier</th>\n",
       "      <th>clf hidden size factor</th>\n",
       "      <th>emb size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>loss</th>\n",
       "      <th>group</th>\n",
       "      <th>Pearson R</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>attribute</th>\n",
       "      <th>group_standardized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.218510</td>\n",
       "      <td>0.260668</td>\n",
       "      <td>0.178546</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>weighted mean</td>\n",
       "      <td>0.166886</td>\n",
       "      <td>0.224006</td>\n",
       "      <td>0.113047</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean (p &lt; 0.05)</td>\n",
       "      <td>0.329735</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.269221</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.228754</td>\n",
       "      <td>0.053363</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sbic</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>bert</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>black</td>\n",
       "      <td>0.676848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731941</td>\n",
       "      <td>[18, 26, 28, 29]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.765516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8199</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>weighted mean</td>\n",
       "      <td>0.765516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean (p &lt; 0.05)</td>\n",
       "      <td>0.765516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.772165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947267</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>twitterAAE</td>\n",
       "      <td>deepseek-ai/deepseek-llm-7b-chat</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.758867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937766</td>\n",
       "      <td>[12, 10, 17, 10]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17111 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset                             model model type architecture  \\\n",
       "0           sbic                 bert-base-uncased       bert      encoder   \n",
       "1           sbic                 bert-base-uncased       bert      encoder   \n",
       "2           sbic                 bert-base-uncased       bert      encoder   \n",
       "3           sbic                 bert-base-uncased       bert      encoder   \n",
       "4           sbic                 bert-base-uncased       bert      encoder   \n",
       "...          ...                               ...        ...          ...   \n",
       "8198  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8199  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8200  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8201  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "8202  twitterAAE  deepseek-ai/deepseek-llm-7b-chat   deepseek      encoder   \n",
       "\n",
       "     pooling classifier  clf hidden size factor  emb size optimizer  lr  \\\n",
       "0       mean     linear                    -1.0       768     Salsa NaN   \n",
       "1       mean     linear                    -1.0       768     Salsa NaN   \n",
       "2       mean     linear                    -1.0       768     Salsa NaN   \n",
       "3       mean     linear                    -1.0       768     Salsa NaN   \n",
       "4       mean     linear                    -1.0       768     Salsa NaN   \n",
       "...      ...        ...                     ...       ...       ...  ..   \n",
       "8198    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8199    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8200    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8201    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "8202    mean       MLP2                     1.0      4096     Salsa NaN   \n",
       "\n",
       "                   loss            group  Pearson R    pvalue    PR-AUC  \\\n",
       "0     BCEWithLogitsLoss             mean   0.218510  0.260668  0.178546   \n",
       "1     BCEWithLogitsLoss    weighted mean   0.166886  0.224006  0.113047   \n",
       "2     BCEWithLogitsLoss  mean (p < 0.05)   0.329735  0.000078  0.269221   \n",
       "3     BCEWithLogitsLoss            white   0.130812  0.228754  0.053363   \n",
       "4     BCEWithLogitsLoss            black   0.676848  0.000000  0.731941   \n",
       "...                 ...              ...        ...       ...       ...   \n",
       "8198  BCEWithLogitsLoss             mean   0.765516  0.000000  0.942517   \n",
       "8199  BCEWithLogitsLoss    weighted mean   0.765516  0.000000  0.942517   \n",
       "8200  BCEWithLogitsLoss  mean (p < 0.05)   0.765516  0.000000  0.942517   \n",
       "8201  BCEWithLogitsLoss               aa   0.772165  0.000000  0.947267   \n",
       "8202  BCEWithLogitsLoss            white   0.758867  0.000000  0.937766   \n",
       "\n",
       "                Epochs  attribute group_standardized  \n",
       "0     [18, 26, 28, 29]        NaN                NaN  \n",
       "1     [18, 26, 28, 29]        NaN                NaN  \n",
       "2     [18, 26, 28, 29]        NaN                NaN  \n",
       "3     [18, 26, 28, 29]  ethnicity              white  \n",
       "4     [18, 26, 28, 29]  ethnicity              black  \n",
       "...                ...        ...                ...  \n",
       "8198  [12, 10, 17, 10]        NaN                NaN  \n",
       "8199  [12, 10, 17, 10]        NaN                NaN  \n",
       "8200  [12, 10, 17, 10]        NaN                NaN  \n",
       "8201  [12, 10, 17, 10]  ethnicity              black  \n",
       "8202  [12, 10, 17, 10]  ethnicity              white  \n",
       "\n",
       "[17111 rows x 18 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add protected attribute\n",
    "LBL_CONFIG = '../configs/concept_transfer/label_matches_complete.yaml'\n",
    "with open(LBL_CONFIG, 'r') as ff:\n",
    "    label_match_config = yaml.safe_load(ff)\n",
    "\n",
    "group_to_standardized_lbl = {grp: group for attr, lookup in label_match_config.items() for group, labels in lookup.items() for grp in labels}\n",
    "group_to_attr = {grp: attr for attr, lookup in label_match_config.items() for group, labels in lookup.items() for grp in labels}\n",
    "\n",
    "df['attribute'] = df['group'].map(group_to_attr)\n",
    "df['group_standardized'] = df['group'].map(group_to_standardized_lbl)\n",
    "\n",
    "df_transfer['attribute'] = df_transfer['group'].map(group_to_attr)\n",
    "df_transfer['group_standardized'] = df_transfer['group'].map(group_to_standardized_lbl)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79da9b-8dbc-49d9-8877-5e209a5c2dbf",
   "metadata": {},
   "source": [
    "#### 2) Add number of samples/ group ratio to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "22b16b13-4a4f-4836-87ec-fb262666bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(ds: CustomDataset):\n",
    "    # protected groups\n",
    "    group_dist = {'n samples': {group: 0 for group in ds.group_names} for split in ds.splits}\n",
    "    group_df = pd.DataFrame(group_dist)\n",
    "\n",
    "    split = 'train' if 'train' in ds.splits else ds.splits[0]\n",
    "    for i, group in enumerate(ds.group_names):\n",
    "        group_df.loc[group, 'n samples'] = np.sum(ds.protected_groups[split][:,i])\n",
    "    total = ds.n_samples[split]\n",
    "    group_df['ratio'] = group_df['n samples']*100/total\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cfaefc6c-9c44-4639-9a99-c031c6eba511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load crowspairs\n",
      "compute class weights for split test\n",
      "test (3016, 1)\n",
      "load twitterAAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_500632/1279798183.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '10.444297082228116' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[((df['dataset'] == dataset) & (df['group'] == grp)), 'group ratio'] = grp_df.loc[grp, 'ratio']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute class weights for split test\n",
      "test (100000, 1)\n",
      "load JigsawBias with option: single-class\n",
      "compute class weights for split train\n",
      "compute class weights for split dev\n",
      "compute class weights for split test\n",
      "train (357019, 2)\n",
      "dev (18876, 2)\n",
      "test (19042, 2)\n",
      "Loading Implicit Hate dataset with option all from: ../../../../data/implicit-hate-corpus/\n",
      "compute class weights for split test\n",
      "test (21480, 10)\n",
      "load BIOS with option supervised\n",
      "compute class weights for split train\n",
      "compute class weights for split test\n",
      "compute class weights for split dev\n",
      "train (7017, 10)\n",
      "test (2500, 10)\n",
      "dev (1046, 10)\n",
      "load Stereoset with option both\n",
      "Load inter- and intrasentence samples and merge them to one dataset\n",
      "compute class weights for split val\n",
      "val (12687, 1)\n",
      "load winoqueer\n",
      "compute class weights for split test\n",
      "test (91080, 1)\n",
      "load SBIC with local file: ../../../../data/filtered_sbic_minority_overview.csv\n",
      "compute class weights for split train\n",
      "compute class weights for split test\n",
      "compute class weights for split dev\n",
      "train (35933, 3)\n",
      "test (4705, 3)\n",
      "dev (4680, 3)\n"
     ]
    }
   ],
   "source": [
    "local_dirs = {'bios-supervised': '../../../../data/bios_huggingface_merge.pkl',\n",
    "              'jigsaw': '../../../../data/jigsaw_unintended_bias',\n",
    "              'sbic': '../../../../data/filtered_sbic_minority_overview.csv',\n",
    "              'twitterAAE': None,\n",
    "              'implicit_hate': '../../../../data/implicit-hate-corpus/',\n",
    "              'winoqueer': '../../../../data/winoqueer_final.csv',\n",
    "              'crows_pairs': None,\n",
    "              'stereoset': None\n",
    "             }\n",
    "\n",
    "df['group ratio'] = 0\n",
    "\n",
    "for dataset in set(df['dataset']):\n",
    "    ds = get_dataset(dataset, local_dirs[dataset])\n",
    "    for split in ds.splits:\n",
    "        print(split, ds.labels[split].shape)\n",
    "    grp_df = get_dataset_stats(ds)\n",
    "    for grp in grp_df.index:\n",
    "        df.loc[((df['dataset'] == dataset) & (df['group'] == grp)), 'group samples'] = grp_df.loc[grp, 'n samples']\n",
    "        df.loc[((df['dataset'] == dataset) & (df['group'] == grp)), 'group ratio'] = grp_df.loc[grp, 'ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "be533483-c0b2-4732-a2fb-e2632d322fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace dataset names with abbreviations\n",
    "lookup = {\n",
    "    \"bios-supervised\": \"BIOS\",\n",
    "    \"bios_sup\": \"BIOS\",\n",
    "    \"crows_pairs\": \"CrowSPairs\",\n",
    "    \"crowspairs\": \"CrowSPairs\",\n",
    "    \"implicit_hate\": \"ImplicitHate\",\n",
    "    \"jigsaw\": \"Jigsaw\",\n",
    "    \"sbic\": \"SBIC\",\n",
    "    \"stereoset\": \"StereoSet\",\n",
    "    \"twitterAAE\": \"TwitterAAE\",\n",
    "    \"winoqueer\": \"WinoQueer\"\n",
    "}\n",
    "\n",
    "# Option 1: using replace\n",
    "df[\"dataset\"] = df[\"dataset\"].replace(lookup)\n",
    "df_transfer[\"dataset train\"] = df_transfer[\"dataset train\"].replace(lookup)\n",
    "df_transfer[\"dataset test\"] = df_transfer[\"dataset test\"].replace(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "72801a28-a107-4322-9822-78f68079d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace model family names with abbreviations\n",
    "lookup = {\n",
    "    \"text-embedding-3\": \"text-emb.\",\n",
    "    \"deberta-v2\": \"deberta\"\n",
    "}\n",
    "\n",
    "# Option 1: using replace\n",
    "df[\"model type\"] = df[\"model type\"].replace(lookup)\n",
    "df_transfer[\"model type\"] = df_transfer[\"model type\"].replace(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614aae4d-37df-4855-a2e5-594b3ee06e4f",
   "metadata": {},
   "source": [
    "#### 3) Compare success rates of Classifiers, Datasets and Models/ Pooling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a359c2db-bb91-4830-8fa3-44789ecb9365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf success rates:\n",
      "MLP2\t\t0.74\n",
      "linear\t\t0.81\n",
      "\n",
      "dataset success rates:\n",
      "SBIC\t\t0.71\n",
      "BIOS\t\t0.96\n",
      "StereoSet\t\t0.95\n",
      "ImplicitHate\t\t0.70\n",
      "Jigsaw\t\t0.75\n",
      "CrowSPairs\t\t0.96\n",
      "TwitterAAE\t\t0.38\n",
      "WinoQueer\t\t0.77\n",
      "\n",
      "model success rates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EleutherAI/pythia-1b</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-base-v2</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Llama-3.2-1B-Instruct</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/deberta-v3-base</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-base-uncased</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EleutherAI/pythia-160m</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta-base</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilroberta-base</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/opt-1.3b</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet/xlnet-base-cased</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EleutherAI/pythia-410m</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-embedding-3-large</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-embedding-3-small</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/deberta-v3-large</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Llama-3.2-3B</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/opt-125m</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/deberta-v3-small</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EleutherAI/pythia-1.4b</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-ai/deepseek-llm-7b-base</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Llama-3.2-3B-Instruct</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet/xlnet-large-cased</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek-ai/deepseek-llm-7b-chat</th>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta-large</th>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Llama-3.2-1B</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mean     cls\n",
       "EleutherAI/pythia-1b                       0.8750  0.5000\n",
       "albert-base-v2                             0.8125  0.8125\n",
       "meta-llama/Llama-3.2-1B-Instruct           1.0000  0.9375\n",
       "microsoft/deberta-v3-base                  0.8750  0.7500\n",
       "distilbert-base-uncased                    0.8125  0.7500\n",
       "bert-large-uncased                         0.8125  0.7500\n",
       "bert-base-uncased                          0.8750  0.8125\n",
       "EleutherAI/pythia-160m                     0.5000  0.3750\n",
       "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  1.0000  0.6250\n",
       "roberta-base                               0.8750  0.7500\n",
       "distilroberta-base                         0.8125  0.8750\n",
       "facebook/opt-1.3b                          1.0000  0.5625\n",
       "xlnet/xlnet-base-cased                     1.0000  0.9375\n",
       "EleutherAI/pythia-410m                     0.8750  0.5000\n",
       "text-embedding-3-large                     0.8750     NaN\n",
       "text-embedding-3-small                     0.9375     NaN\n",
       "microsoft/deberta-v3-large                 0.9375  0.6875\n",
       "meta-llama/Llama-3.2-3B                    1.0000  0.5625\n",
       "facebook/opt-125m                          1.0000  0.5625\n",
       "albert-large-v2                            0.8125  0.7500\n",
       "microsoft/deberta-v3-small                 0.9375  0.7500\n",
       "EleutherAI/pythia-1.4b                     0.8750  0.5625\n",
       "deepseek-ai/deepseek-llm-7b-base           0.9375  0.5000\n",
       "meta-llama/Llama-3.2-3B-Instruct           1.0000  0.2500\n",
       "xlnet/xlnet-large-cased                    0.9375  0.8125\n",
       "deepseek-ai/deepseek-llm-7b-chat           0.9375  0.0000\n",
       "roberta-large                              0.8750  0.6250\n",
       "meta-llama/Llama-3.2-1B                    0.8125  0.8125"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = {key: {'failed': 0, 'succeded': 0} for key in set(df['classifier'])}\n",
    "datasets = {key: {'failed': 0, 'succeded': 0} for key in set(df['dataset'])}\n",
    "poolings = {key: {'failed': 0, 'succeded': 0} for key in set(df['pooling'])}# if not pd.isna(key)} # TODO distinguish models/ placeholder for text-embedding models instead of nan\n",
    "models = {key: {pooling: {'failed': 0, 'succeded': 0} for pooling in poolings.keys()} for key in set(df['model'])}\n",
    "\n",
    "for model in models.keys():\n",
    "    count = 0\n",
    "    for clf in clfs.keys():\n",
    "        for dataset in datasets.keys():\n",
    "            cur_poolings = ['unknown'] if 'text-embedding' in model else ['mean', 'cls']\n",
    "            for pooling in cur_poolings:\n",
    "                count += 1\n",
    "                df_sel = df[(df['dataset'] == dataset) & (df['model'] == model) & (df['classifier'] == clf) & (df['pooling'] == pooling)]\n",
    "                if len(df_sel) == 0:\n",
    "                    clfs[clf]['failed'] += 1\n",
    "                    poolings[pooling]['failed'] += 1\n",
    "                    models[model][pooling]['failed'] += 1\n",
    "                    datasets[dataset]['failed'] += 1    \n",
    "                else:\n",
    "                    clfs[clf]['succeded'] += 1\n",
    "                    poolings[pooling]['succeded'] += 1\n",
    "                    models[model][pooling]['succeded'] += 1\n",
    "                    datasets[dataset]['succeded'] += 1\n",
    "    #print(model, count)\n",
    "\n",
    "model_rates = {'mean': {}, 'cls': {}}\n",
    "for k,v in models.items():\n",
    "    if 'text-embedding' in k:\n",
    "        ratio = v['unknown']['succeded']/(v['unknown']['succeded']+v['unknown']['failed'])\n",
    "        #print(\"%s\\t\\t%.2f\" % (k, ratio))\n",
    "        model_rates['mean'][k] = ratio\n",
    "    else:\n",
    "        mean_ratio = v['mean']['succeded']/(v['mean']['succeded']+v['mean']['failed'])\n",
    "        cls_ratio = v['cls']['succeded']/(v['cls']['succeded']+v['cls']['failed'])\n",
    "        #print(\"%s\\t\\tmean: %.2f\\tcls: %.2f\" % (k, mean_ratio, cls_ratio))\n",
    "        model_rates['mean'][k] = mean_ratio\n",
    "        model_rates['cls'][k] = cls_ratio\n",
    "\n",
    "print(\"clf success rates:\")\n",
    "for clf, v in clfs.items():\n",
    "    ratio = v['succeded']/(v['succeded']+v['failed'])\n",
    "    print(\"%s\\t\\t%.2f\" % (clf, ratio))\n",
    "\n",
    "print(\"\\ndataset success rates:\")\n",
    "for k, v in datasets.items():\n",
    "    ratio = v['succeded']/(v['succeded']+v['failed'])\n",
    "    print(\"%s\\t\\t%.2f\" % (k, ratio))\n",
    "\n",
    "print(\"\\nmodel success rates:\")\n",
    "pd.DataFrame(model_rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af3cd2-2197-4a4a-9bc1-178b91aa67ac",
   "metadata": {},
   "source": [
    "#### 4) Evaluate average PR-AUC and number of statistical significant correlations per model and pooling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "80876829-b044-4157-9774-7d7b9a609b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups = df[(df['group'] != 'mean') & (df['group'] != 'weighted mean') & (df['group'] != 'mean (p < 0.05)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6cbf3023-8d13-4f97-be05-5b0c678afb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/pythia-1b\n",
      "mean: 352 results, 290 correlations statistical significant, mean AUC: 0.625\n",
      "cls: 194 results, 87 correlations statistical significant, mean AUC: 0.566\n",
      "-> mean\n",
      "\n",
      "albert-base-v2\n",
      "mean: 277 results, 187 correlations statistical significant, mean AUC: 0.571\n",
      "cls: 333 results, 155 correlations statistical significant, mean AUC: 0.454\n",
      "-> mean\n",
      "\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "mean: 356 results, 264 correlations statistical significant, mean AUC: 0.660\n",
      "cls: 337 results, 216 correlations statistical significant, mean AUC: 0.587\n",
      "-> mean\n",
      "\n",
      "microsoft/deberta-v3-base\n",
      "mean: 318 results, 161 correlations statistical significant, mean AUC: 0.546\n",
      "cls: 224 results, 43 correlations statistical significant, mean AUC: 0.418\n",
      "-> mean\n",
      "\n",
      "distilbert-base-uncased\n",
      "mean: 277 results, 211 correlations statistical significant, mean AUC: 0.630\n",
      "cls: 258 results, 158 correlations statistical significant, mean AUC: 0.639\n",
      "\n",
      "bert-large-uncased\n",
      "mean: 277 results, 196 correlations statistical significant, mean AUC: 0.615\n",
      "cls: 258 results, 154 correlations statistical significant, mean AUC: 0.564\n",
      "-> mean\n",
      "\n",
      "bert-base-uncased\n",
      "mean: 352 results, 269 correlations statistical significant, mean AUC: 0.589\n",
      "cls: 333 results, 221 correlations statistical significant, mean AUC: 0.593\n",
      "\n",
      "EleutherAI/pythia-160m\n",
      "mean: 132 results, 20 correlations statistical significant, mean AUC: 0.351\n",
      "cls: 177 results, 11 correlations statistical significant, mean AUC: 0.131\n",
      "-> mean\n",
      "\n",
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "mean: 356 results, 278 correlations statistical significant, mean AUC: 0.655\n",
      "cls: 288 results, 209 correlations statistical significant, mean AUC: 0.536\n",
      "-> mean\n",
      "\n",
      "roberta-base\n",
      "mean: 352 results, 260 correlations statistical significant, mean AUC: 0.589\n",
      "cls: 258 results, 157 correlations statistical significant, mean AUC: 0.602\n",
      "\n",
      "distilroberta-base\n",
      "mean: 277 results, 203 correlations statistical significant, mean AUC: 0.638\n",
      "cls: 352 results, 198 correlations statistical significant, mean AUC: 0.557\n",
      "-> mean\n",
      "\n",
      "facebook/opt-1.3b\n",
      "mean: 356 results, 301 correlations statistical significant, mean AUC: 0.673\n",
      "cls: 213 results, 93 correlations statistical significant, mean AUC: 0.463\n",
      "-> mean\n",
      "\n",
      "xlnet/xlnet-base-cased\n",
      "mean: 356 results, 223 correlations statistical significant, mean AUC: 0.592\n",
      "cls: 281 results, 134 correlations statistical significant, mean AUC: 0.522\n",
      "-> mean\n",
      "\n",
      "EleutherAI/pythia-410m\n",
      "mean: 352 results, 233 correlations statistical significant, mean AUC: 0.590\n",
      "cls: 194 results, 65 correlations statistical significant, mean AUC: 0.365\n",
      "-> mean\n",
      "\n",
      "microsoft/deberta-v3-large\n",
      "mean: 281 results, 126 correlations statistical significant, mean AUC: 0.526\n",
      "cls: 149 results, 34 correlations statistical significant, mean AUC: 0.413\n",
      "-> mean\n",
      "\n",
      "meta-llama/Llama-3.2-3B\n",
      "mean: 356 results, 258 correlations statistical significant, mean AUC: 0.657\n",
      "cls: 269 results, 56 correlations statistical significant, mean AUC: 0.431\n",
      "-> mean\n",
      "\n",
      "facebook/opt-125m\n",
      "mean: 356 results, 296 correlations statistical significant, mean AUC: 0.651\n",
      "cls: 213 results, 59 correlations statistical significant, mean AUC: 0.426\n",
      "-> mean\n",
      "\n",
      "albert-large-v2\n",
      "mean: 277 results, 170 correlations statistical significant, mean AUC: 0.576\n",
      "cls: 314 results, 116 correlations statistical significant, mean AUC: 0.452\n",
      "-> mean\n",
      "\n",
      "microsoft/deberta-v3-small\n",
      "mean: 281 results, 140 correlations statistical significant, mean AUC: 0.548\n",
      "cls: 224 results, 76 correlations statistical significant, mean AUC: 0.429\n",
      "-> mean\n",
      "\n",
      "EleutherAI/pythia-1.4b\n",
      "mean: 352 results, 280 correlations statistical significant, mean AUC: 0.625\n",
      "cls: 213 results, 137 correlations statistical significant, mean AUC: 0.484\n",
      "-> mean\n",
      "\n",
      "deepseek-ai/deepseek-llm-7b-base\n",
      "mean: 281 results, 153 correlations statistical significant, mean AUC: 0.658\n",
      "cls: 194 results, 31 correlations statistical significant, mean AUC: 0.463\n",
      "-> mean\n",
      "\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "mean: 356 results, 278 correlations statistical significant, mean AUC: 0.671\n",
      "cls: 154 results, 112 correlations statistical significant, mean AUC: 0.604\n",
      "-> mean\n",
      "\n",
      "xlnet/xlnet-large-cased\n",
      "mean: 354 results, 159 correlations statistical significant, mean AUC: 0.509\n",
      "cls: 243 results, 112 correlations statistical significant, mean AUC: 0.507\n",
      "-> mean\n",
      "\n",
      "deepseek-ai/deepseek-llm-7b-chat\n",
      "mean: 281 results, 154 correlations statistical significant, mean AUC: 0.663\n",
      "cls: 0 results, 0 correlations statistical significant, mean AUC: nan\n",
      "\n",
      "roberta-large\n",
      "mean: 352 results, 222 correlations statistical significant, mean AUC: 0.608\n",
      "cls: 220 results, 77 correlations statistical significant, mean AUC: 0.571\n",
      "-> mean\n",
      "\n",
      "meta-llama/Llama-3.2-1B\n",
      "mean: 277 results, 145 correlations statistical significant, mean AUC: 0.620\n",
      "cls: 333 results, 144 correlations statistical significant, mean AUC: 0.468\n",
      "-> mean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = set(df['model'])\n",
    "\n",
    "# get best pooling method per model\n",
    "for model in models:\n",
    "    if 'text-embedding' in model:\n",
    "        continue\n",
    "    df_mean = df_groups[(df_groups['model'] == model) & (df_groups['pooling'] == 'mean')]\n",
    "    df_cls = df_groups[(df_groups['model'] == model) & (df_groups['pooling'] == 'cls')]\n",
    "\n",
    "    agg_mean = aggregate_results(df_mean, sel_cols=['dataset', 'classifier'], target_col='PR-AUC')\n",
    "    agg_cls = aggregate_results(df_cls, sel_cols=['dataset', 'classifier'], target_col='PR-AUC')\n",
    "    auc_mean = np.mean(agg_mean['PR-AUC'])\n",
    "    auc_cls = np.mean(agg_cls['PR-AUC'])\n",
    "    n_sign_mean = (df_mean['pvalue'] < 0.01).sum()\n",
    "    n_sign_cls = (df_cls['pvalue'] < 0.01).sum()\n",
    "\n",
    "    print(model)\n",
    "    print(\"mean: %i results, %i correlations statistical significant, mean AUC: %.3f\" % (len(df_mean), n_sign_mean, auc_mean))\n",
    "    print(\"cls: %i results, %i correlations statistical significant, mean AUC: %.3f\" % (len(df_cls), n_sign_cls, auc_cls))\n",
    "\n",
    "    if (n_sign_mean > n_sign_cls and auc_mean > auc_cls):\n",
    "        print(\"-> mean\")\n",
    "    elif (n_sign_mean < n_sign_cls and auc_mean < auc_cls):\n",
    "        print(\"-> cls\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4abe2-fa66-4b0a-b4ef-896814e2519c",
   "metadata": {},
   "source": [
    "#### 5) Filter for models and pooling method\n",
    "\n",
    "Exclude blacklisted model(s) and pick the better pooling method (mean) per model. Remove unused mean results and save preprocessed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c83d0fa9-de5a-4939-9a73-250540eec12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text-embedding-3-small': 'unknown',\n",
       " 'text-embedding-3-large': 'unknown',\n",
       " 'EleutherAI/pythia-1b': 'mean',\n",
       " 'albert-base-v2': 'mean',\n",
       " 'meta-llama/Llama-3.2-1B-Instruct': 'mean',\n",
       " 'microsoft/deberta-v3-base': 'mean',\n",
       " 'distilbert-base-uncased': 'mean',\n",
       " 'bert-large-uncased': 'mean',\n",
       " 'bert-base-uncased': 'mean',\n",
       " 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B': 'mean',\n",
       " 'roberta-base': 'mean',\n",
       " 'distilroberta-base': 'mean',\n",
       " 'facebook/opt-1.3b': 'mean',\n",
       " 'xlnet/xlnet-base-cased': 'mean',\n",
       " 'EleutherAI/pythia-410m': 'mean',\n",
       " 'microsoft/deberta-v3-large': 'mean',\n",
       " 'meta-llama/Llama-3.2-3B': 'mean',\n",
       " 'facebook/opt-125m': 'mean',\n",
       " 'albert-large-v2': 'mean',\n",
       " 'microsoft/deberta-v3-small': 'mean',\n",
       " 'EleutherAI/pythia-1.4b': 'mean',\n",
       " 'deepseek-ai/deepseek-llm-7b-base': 'mean',\n",
       " 'meta-llama/Llama-3.2-3B-Instruct': 'mean',\n",
       " 'xlnet/xlnet-large-cased': 'mean',\n",
       " 'deepseek-ai/deepseek-llm-7b-chat': 'mean',\n",
       " 'roberta-large': 'mean',\n",
       " 'meta-llama/Llama-3.2-1B': 'mean'}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the following we only look into the best performing pooling method per model\n",
    "pooling_per_model = {'text-embedding-3-small': 'unknown',\n",
    "                     'text-embedding-3-large': 'unknown'\n",
    "                    }\n",
    "\n",
    "model_blacklist = [\"EleutherAI/pythia-160m\"]\n",
    "\n",
    "for model in models:\n",
    "    if model not in pooling_per_model.keys() and model not in model_blacklist:\n",
    "        pooling_per_model[model] = 'mean'\n",
    "pooling_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f685ddb7-25c9-4003-be6d-c3e00634fda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>model type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>pooling</th>\n",
       "      <th>classifier</th>\n",
       "      <th>clf hidden size factor</th>\n",
       "      <th>emb size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>loss</th>\n",
       "      <th>group</th>\n",
       "      <th>Pearson R</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>attribute</th>\n",
       "      <th>group_standardized</th>\n",
       "      <th>group ratio</th>\n",
       "      <th>group samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>text-emb.</td>\n",
       "      <td>embedder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.353655</td>\n",
       "      <td>1.626997e-01</td>\n",
       "      <td>0.314080</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>text-emb.</td>\n",
       "      <td>embedder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>white</td>\n",
       "      <td>0.317888</td>\n",
       "      <td>4.304603e-236</td>\n",
       "      <td>0.193677</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>white</td>\n",
       "      <td>0.333955</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>text-emb.</td>\n",
       "      <td>embedder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>black</td>\n",
       "      <td>0.767017</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.855652</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>black</td>\n",
       "      <td>8.312693</td>\n",
       "      <td>2987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>text-emb.</td>\n",
       "      <td>embedder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>asian</td>\n",
       "      <td>0.502718</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.497592</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>asian</td>\n",
       "      <td>1.775527</td>\n",
       "      <td>638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>text-emb.</td>\n",
       "      <td>embedder</td>\n",
       "      <td>unknown</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>non-white</td>\n",
       "      <td>0.762215</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.851496</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>non_white</td>\n",
       "      <td>8.479670</td>\n",
       "      <td>3047.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>meta-llama/Llama-3.2-1B</td>\n",
       "      <td>llama</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2048</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>Gay</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>5.935441</td>\n",
       "      <td>5406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>meta-llama/Llama-3.2-1B</td>\n",
       "      <td>llama</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2048</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>16.214317</td>\n",
       "      <td>14768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>meta-llama/Llama-3.2-1B</td>\n",
       "      <td>llama</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2048</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>Straight</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>heterosexual</td>\n",
       "      <td>16.214317</td>\n",
       "      <td>14768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9920</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>meta-llama/Llama-3.2-1B</td>\n",
       "      <td>llama</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2048</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>Cis</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>cis</td>\n",
       "      <td>8.785683</td>\n",
       "      <td>8002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9921</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>meta-llama/Llama-3.2-1B</td>\n",
       "      <td>llama</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2048</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>Cisgender</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[10, 10, 10, 10]</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>cis</td>\n",
       "      <td>8.785683</td>\n",
       "      <td>8002.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9138 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset                    model model type architecture  pooling  \\\n",
       "0          SBIC   text-embedding-3-small  text-emb.     embedder  unknown   \n",
       "3          SBIC   text-embedding-3-small  text-emb.     embedder  unknown   \n",
       "4          SBIC   text-embedding-3-small  text-emb.     embedder  unknown   \n",
       "5          SBIC   text-embedding-3-small  text-emb.     embedder  unknown   \n",
       "6          SBIC   text-embedding-3-small  text-emb.     embedder  unknown   \n",
       "...         ...                      ...        ...          ...      ...   \n",
       "9917  WinoQueer  meta-llama/Llama-3.2-1B      llama      decoder     mean   \n",
       "9918  WinoQueer  meta-llama/Llama-3.2-1B      llama      decoder     mean   \n",
       "9919  WinoQueer  meta-llama/Llama-3.2-1B      llama      decoder     mean   \n",
       "9920  WinoQueer  meta-llama/Llama-3.2-1B      llama      decoder     mean   \n",
       "9921  WinoQueer  meta-llama/Llama-3.2-1B      llama      decoder     mean   \n",
       "\n",
       "     classifier  clf hidden size factor  emb size optimizer  lr  \\\n",
       "0        linear                    -1.0      1536     Salsa NaN   \n",
       "3        linear                    -1.0      1536     Salsa NaN   \n",
       "4        linear                    -1.0      1536     Salsa NaN   \n",
       "5        linear                    -1.0      1536     Salsa NaN   \n",
       "6        linear                    -1.0      1536     Salsa NaN   \n",
       "...         ...                     ...       ...       ...  ..   \n",
       "9917       MLP2                     1.0      2048     Salsa NaN   \n",
       "9918       MLP2                     1.0      2048     Salsa NaN   \n",
       "9919       MLP2                     1.0      2048     Salsa NaN   \n",
       "9920       MLP2                     1.0      2048     Salsa NaN   \n",
       "9921       MLP2                     1.0      2048     Salsa NaN   \n",
       "\n",
       "                   loss         group  Pearson R         pvalue    PR-AUC  \\\n",
       "0     BCEWithLogitsLoss          mean   0.353655   1.626997e-01  0.314080   \n",
       "3     BCEWithLogitsLoss         white   0.317888  4.304603e-236  0.193677   \n",
       "4     BCEWithLogitsLoss         black   0.767017   0.000000e+00  0.855652   \n",
       "5     BCEWithLogitsLoss         asian   0.502718   0.000000e+00  0.497592   \n",
       "6     BCEWithLogitsLoss     non-white   0.762215   0.000000e+00  0.851496   \n",
       "...                 ...           ...        ...            ...       ...   \n",
       "9917  BCEWithLogitsLoss           Gay   1.000000   0.000000e+00  1.000000   \n",
       "9918  BCEWithLogitsLoss  Heterosexual   1.000000   0.000000e+00  1.000000   \n",
       "9919  BCEWithLogitsLoss      Straight   0.999999   0.000000e+00  1.000000   \n",
       "9920  BCEWithLogitsLoss           Cis   0.999994   0.000000e+00  1.000000   \n",
       "9921  BCEWithLogitsLoss     Cisgender   0.999998   0.000000e+00  1.000000   \n",
       "\n",
       "                Epochs                  attribute group_standardized  \\\n",
       "0     [10, 10, 10, 10]                        NaN                NaN   \n",
       "3     [10, 10, 10, 10]                  ethnicity              white   \n",
       "4     [10, 10, 10, 10]                  ethnicity              black   \n",
       "5     [10, 10, 10, 10]                  ethnicity              asian   \n",
       "6     [10, 10, 10, 10]                  ethnicity          non_white   \n",
       "...                ...                        ...                ...   \n",
       "9917  [10, 10, 10, 10]  gender_sexual_orientation         homosexual   \n",
       "9918  [10, 10, 10, 10]  gender_sexual_orientation       heterosexual   \n",
       "9919  [10, 10, 10, 10]  gender_sexual_orientation       heterosexual   \n",
       "9920  [10, 10, 10, 10]  gender_sexual_orientation                cis   \n",
       "9921  [10, 10, 10, 10]  gender_sexual_orientation                cis   \n",
       "\n",
       "      group ratio  group samples  \n",
       "0        0.000000            NaN  \n",
       "3        0.333955          120.0  \n",
       "4        8.312693         2987.0  \n",
       "5        1.775527          638.0  \n",
       "6        8.479670         3047.0  \n",
       "...           ...            ...  \n",
       "9917     5.935441         5406.0  \n",
       "9918    16.214317        14768.0  \n",
       "9919    16.214317        14768.0  \n",
       "9920     8.785683         8002.0  \n",
       "9921     8.785683         8002.0  \n",
       "\n",
       "[9138 rows x 20 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for model, pooling in pooling_per_model.items():\n",
    "    df_model = df[(df['model'] == model) & (df['pooling'] == pooling)]\n",
    "    dfs.append(df_model)\n",
    "df_sel = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_sel = df_sel[(df_sel['group'] != 'weighted mean') & (df_sel['group'] != 'mean (p < 0.05)')]\n",
    "df_sel.to_csv(PROCESSED_RESULTS_FILE)\n",
    "\n",
    "df_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ed7f6-4b0a-4482-9517-7389b7861187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265c7d6-1695-428d-b189-3592d4e4f562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6867a2eb-431d-4901-bbb5-dfb82ddfbf75",
   "metadata": {},
   "source": [
    "### Transfer experiment: Check that results exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "86db99fa-4169-4a90-a9df-eb4fd5acfc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 5157 results for linear\n",
      "got 6450 results for MLP\n"
     ]
    }
   ],
   "source": [
    "df_lin = df_transfer[df_transfer['classifier'] == 'linear']\n",
    "df_mlp = df_transfer[df_transfer['classifier'] == 'MLP2']\n",
    "\n",
    "print(\"got %i results for linear\" % len(df_lin))\n",
    "print(\"got %i results for MLP\" % len(df_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7b1a2250-ed0c-4d15-bd5c-2bc1104bc55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SBIC', 'BIOS', 'StereoSet', 'ImplicitHate', 'Jigsaw', 'CrowSPairs', 'TwitterAAE', 'WinoQueer']\n",
      "['deepseek-ai/deepseek-llm-7b-base', 'albert-large-v2', 'xlnet/xlnet-large-cased', 'deepseek-ai/deepseek-llm-7b-chat', 'albert-base-v2', 'EleutherAI/pythia-410m', 'roberta-large', 'microsoft/deberta-v3-large', 'meta-llama/Llama-3.2-1B']\n",
      "missing:   SBIC \t-> StereoSet got 9 MLP and 8 linear reults\n",
      "missing:   SBIC \t-> ImplicitHate got 9 MLP and 8 linear reults\n",
      "missing:   SBIC \t-> Jigsaw got 9 MLP and 8 linear reults\n",
      "missing:   SBIC \t-> TwitterAAE got 9 MLP and 8 linear reults\n",
      "missing:   SBIC \t-> WinoQueer got 9 MLP and 8 linear reults\n",
      "missing:   BIOS \t-> TwitterAAE got 0 MLP and 0 linear reults\n",
      "missing:   BIOS \t-> WinoQueer got 0 MLP and 0 linear reults\n",
      "missing:   StereoSet \t-> WinoQueer got 0 MLP and 0 linear reults\n",
      "missing:   TwitterAAE \t-> SBIC got 6 MLP and 5 linear reults\n",
      "missing:   TwitterAAE \t-> BIOS got 0 MLP and 0 linear reults\n",
      "missing:   TwitterAAE \t-> StereoSet got 6 MLP and 5 linear reults\n",
      "missing:   TwitterAAE \t-> ImplicitHate got 6 MLP and 5 linear reults\n",
      "missing:   TwitterAAE \t-> Jigsaw got 6 MLP and 5 linear reults\n",
      "missing:   TwitterAAE \t-> CrowSPairs got 6 MLP and 5 linear reults\n",
      "missing:   TwitterAAE \t-> TwitterAAE got 6 MLP and 5 linear reults\n",
      "missing:   TwitterAAE \t-> WinoQueer got 0 MLP and 0 linear reults\n",
      "missing:   WinoQueer \t-> BIOS got 0 MLP and 0 linear reults\n",
      "missing:   WinoQueer \t-> StereoSet got 0 MLP and 0 linear reults\n",
      "missing:   WinoQueer \t-> TwitterAAE got 0 MLP and 0 linear reults\n"
     ]
    }
   ],
   "source": [
    "datasets = list(set(df_transfer['dataset test']))\n",
    "print(datasets)\n",
    "models = list(set(df_transfer['model']))\n",
    "print(models)\n",
    "\n",
    "finished = []\n",
    "incomplete = []\n",
    "for dtrain in datasets:\n",
    "    for dtest in datasets:\n",
    "        df_lin_ = df_lin[(df_lin['dataset train'] == dtrain) & (df_lin['dataset test'] == dtest)].groupby([\"model\"], as_index=False)[\"PR-AUC\"].mean()\n",
    "        df_mlp_ = df_mlp[(df_mlp['dataset train'] == dtrain) & (df_mlp['dataset test'] == dtest)].groupby([\"model\"], as_index=False)[\"PR-AUC\"].mean()\n",
    "        #agg_res = res.groupby([\"dataset test\", \"dataset train\"], as_index=False)[[\"mlp\", \"linear\", \"diff\"]].mean()\n",
    "        if len(df_lin_) == len(models) and len(df_mlp_) == len(models):\n",
    "            finished.append((dtrain,dtest))\n",
    "        else:\n",
    "            #incomplete.append({'linear': \n",
    "            print(\"missing:   %s \\t-> %s got %i MLP and %i linear reults\" % (dtrain, dtest, len(df_lin_), len(df_mlp_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45beef69-03ee-4531-9d6a-6515370b17f4",
   "metadata": {},
   "source": [
    "Most of these cases (0 results) are expected, because no groups are shared. We miss one model for SBIC and several ones for TwitterAAE.\n",
    "TwitterAAE is removed from transfer results, because the transfer is not meaningful anyway (classifiers do not outperform the random guessing baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d7abdd5c-9a9f-4acb-8435-313bd751fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove TwitterAAE (no meaningful transfer - cannot draw conclusions for our experiment)\n",
    "df_transfer = df_transfer[(df_transfer['dataset train'] != 'TwitterAAE') & (df_transfer['dataset test'] != 'TwitterAAE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "77954035-beb9-40cc-9b3b-34ae0f98b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df_transfer.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f96ed209-260a-4c77-94b1-61d9f190a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset train</th>\n",
       "      <th>dataset test</th>\n",
       "      <th>model</th>\n",
       "      <th>classifier</th>\n",
       "      <th>group</th>\n",
       "      <th>PR-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIOS</td>\n",
       "      <td>BIOS</td>\n",
       "      <td>EleutherAI/pythia-410m</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>female</td>\n",
       "      <td>0.998996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIOS</td>\n",
       "      <td>BIOS</td>\n",
       "      <td>EleutherAI/pythia-410m</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>male</td>\n",
       "      <td>0.999147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIOS</td>\n",
       "      <td>BIOS</td>\n",
       "      <td>EleutherAI/pythia-410m</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.999072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIOS</td>\n",
       "      <td>BIOS</td>\n",
       "      <td>EleutherAI/pythia-410m</td>\n",
       "      <td>linear</td>\n",
       "      <td>female</td>\n",
       "      <td>0.999059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIOS</td>\n",
       "      <td>BIOS</td>\n",
       "      <td>EleutherAI/pythia-410m</td>\n",
       "      <td>linear</td>\n",
       "      <td>male</td>\n",
       "      <td>0.999241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9284</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>linear</td>\n",
       "      <td>Pansexual</td>\n",
       "      <td>0.979868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9285</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>linear</td>\n",
       "      <td>Queer</td>\n",
       "      <td>0.997719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9286</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>linear</td>\n",
       "      <td>Straight</td>\n",
       "      <td>0.998806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9287</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>linear</td>\n",
       "      <td>Transgender</td>\n",
       "      <td>0.989914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>WinoQueer</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>linear</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.990378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9289 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset train dataset test                    model classifier  \\\n",
       "0             BIOS         BIOS   EleutherAI/pythia-410m       MLP2   \n",
       "1             BIOS         BIOS   EleutherAI/pythia-410m       MLP2   \n",
       "2             BIOS         BIOS   EleutherAI/pythia-410m       MLP2   \n",
       "3             BIOS         BIOS   EleutherAI/pythia-410m     linear   \n",
       "4             BIOS         BIOS   EleutherAI/pythia-410m     linear   \n",
       "...            ...          ...                      ...        ...   \n",
       "9284     WinoQueer    WinoQueer  xlnet/xlnet-large-cased     linear   \n",
       "9285     WinoQueer    WinoQueer  xlnet/xlnet-large-cased     linear   \n",
       "9286     WinoQueer    WinoQueer  xlnet/xlnet-large-cased     linear   \n",
       "9287     WinoQueer    WinoQueer  xlnet/xlnet-large-cased     linear   \n",
       "9288     WinoQueer    WinoQueer  xlnet/xlnet-large-cased     linear   \n",
       "\n",
       "            group    PR-AUC  \n",
       "0          female  0.998996  \n",
       "1            male  0.999147  \n",
       "2            mean  0.999072  \n",
       "3          female  0.999059  \n",
       "4            male  0.999241  \n",
       "...           ...       ...  \n",
       "9284    Pansexual  0.979868  \n",
       "9285        Queer  0.997719  \n",
       "9286     Straight  0.998806  \n",
       "9287  Transgender  0.989914  \n",
       "9288         mean  0.990378  \n",
       "\n",
       "[9289 rows x 6 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for some reason we have duplicates -> just merge by mean\n",
    "columns = ['dataset train', 'dataset test', 'model', 'classifier', 'group']\n",
    "duplicates = df_transfer[df_transfer.duplicated(subset=columns, keep=False)]\n",
    "#duplicates\n",
    "\n",
    "score_col = \"PR-AUC\"\n",
    "cols = [c for c in df_transfer.columns if c not in score_col]\n",
    "df_transfer2 = df_transfer.groupby(columns, as_index=False)[score_col].mean()\n",
    "df_transfer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "33f3b6ca-8ae5-4dc2-84ee-decaa409dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfer.to_csv(PROCESSED_TRANSFER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2265daf4-babf-495e-99a7-b60bc49bd452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset train</th>\n",
       "      <th>dataset test</th>\n",
       "      <th>model</th>\n",
       "      <th>model type</th>\n",
       "      <th>architecture</th>\n",
       "      <th>pooling</th>\n",
       "      <th>classifier</th>\n",
       "      <th>clf hidden size factor</th>\n",
       "      <th>emb size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>loss</th>\n",
       "      <th>group</th>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>attribute</th>\n",
       "      <th>group_standardized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.096274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>female</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>male</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>homosexual</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>homosexual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>ImplicitHate</td>\n",
       "      <td>microsoft/deberta-v3-large</td>\n",
       "      <td>deberta</td>\n",
       "      <td>encoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>linear</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>lgbtq+</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>lgbtq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>StereoSet</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>male</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>StereoSet</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>female</td>\n",
       "      <td>0.425631</td>\n",
       "      <td>gender_sexual_orientation</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>StereoSet</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>religion</td>\n",
       "      <td>christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6448</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>StereoSet</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.261707</td>\n",
       "      <td>religion</td>\n",
       "      <td>muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449</th>\n",
       "      <td>SBIC</td>\n",
       "      <td>StereoSet</td>\n",
       "      <td>xlnet/xlnet-large-cased</td>\n",
       "      <td>xlnet</td>\n",
       "      <td>decoder</td>\n",
       "      <td>mean</td>\n",
       "      <td>MLP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Salsa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCEWithLogitsLoss</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.017006</td>\n",
       "      <td>religion</td>\n",
       "      <td>jewish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11130 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset train  dataset test                       model model type  \\\n",
       "18    ImplicitHate  ImplicitHate  microsoft/deberta-v3-large    deberta   \n",
       "19    ImplicitHate  ImplicitHate  microsoft/deberta-v3-large    deberta   \n",
       "20    ImplicitHate  ImplicitHate  microsoft/deberta-v3-large    deberta   \n",
       "21    ImplicitHate  ImplicitHate  microsoft/deberta-v3-large    deberta   \n",
       "22    ImplicitHate  ImplicitHate  microsoft/deberta-v3-large    deberta   \n",
       "...            ...           ...                         ...        ...   \n",
       "6445          SBIC     StereoSet     xlnet/xlnet-large-cased      xlnet   \n",
       "6446          SBIC     StereoSet     xlnet/xlnet-large-cased      xlnet   \n",
       "6447          SBIC     StereoSet     xlnet/xlnet-large-cased      xlnet   \n",
       "6448          SBIC     StereoSet     xlnet/xlnet-large-cased      xlnet   \n",
       "6449          SBIC     StereoSet     xlnet/xlnet-large-cased      xlnet   \n",
       "\n",
       "     architecture pooling classifier  clf hidden size factor  emb size  \\\n",
       "18        encoder    mean     linear                    -1.0      1024   \n",
       "19        encoder    mean     linear                    -1.0      1024   \n",
       "20        encoder    mean     linear                    -1.0      1024   \n",
       "21        encoder    mean     linear                    -1.0      1024   \n",
       "22        encoder    mean     linear                    -1.0      1024   \n",
       "...           ...     ...        ...                     ...       ...   \n",
       "6445      decoder    mean       MLP2                     1.0      1024   \n",
       "6446      decoder    mean       MLP2                     1.0      1024   \n",
       "6447      decoder    mean       MLP2                     1.0      1024   \n",
       "6448      decoder    mean       MLP2                     1.0      1024   \n",
       "6449      decoder    mean       MLP2                     1.0      1024   \n",
       "\n",
       "     optimizer  lr               loss       group    PR-AUC  \\\n",
       "18       Salsa NaN  BCEWithLogitsLoss        mean  0.096274   \n",
       "19       Salsa NaN  BCEWithLogitsLoss      female  0.014929   \n",
       "20       Salsa NaN  BCEWithLogitsLoss        male  0.012225   \n",
       "21       Salsa NaN  BCEWithLogitsLoss  homosexual  0.013935   \n",
       "22       Salsa NaN  BCEWithLogitsLoss      lgbtq+  0.002713   \n",
       "...        ...  ..                ...         ...       ...   \n",
       "6445     Salsa NaN  BCEWithLogitsLoss        male  0.275098   \n",
       "6446     Salsa NaN  BCEWithLogitsLoss      female  0.425631   \n",
       "6447     Salsa NaN  BCEWithLogitsLoss   christian  0.130200   \n",
       "6448     Salsa NaN  BCEWithLogitsLoss      muslim  0.261707   \n",
       "6449     Salsa NaN  BCEWithLogitsLoss      jewish  0.017006   \n",
       "\n",
       "                      attribute group_standardized  \n",
       "18                          NaN                NaN  \n",
       "19    gender_sexual_orientation             female  \n",
       "20    gender_sexual_orientation               male  \n",
       "21    gender_sexual_orientation         homosexual  \n",
       "22    gender_sexual_orientation              lgbtq  \n",
       "...                         ...                ...  \n",
       "6445  gender_sexual_orientation               male  \n",
       "6446  gender_sexual_orientation             female  \n",
       "6447                   religion          christian  \n",
       "6448                   religion             muslim  \n",
       "6449                   religion             jewish  \n",
       "\n",
       "[11130 rows x 16 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2558d4-8783-4b20-82f2-b19100d07de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb0b9a-e492-4cde-8ed5-bd1dd29700a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
