{
"bert-base-uncased": 64,
"bert-large-uncased": 16,
"distilbert-base-uncased": 64,
"roberta-base": 32,
"roberta-large": 16,
"distilroberta-base": 32,
"xlm-roberta-base": 16,
"albert-base-v2": 64,
"albert-large-v2": 16,
"google/electra-base-generator": 64,
"google/electra-large-generator": 64,
"google/electra-base-discriminator": 64,
"google/electra-large-discriminator": 64,
"bert-base-multilingual-uncased": 32,
"GroNLP/hateBERT": 64,
"Twitter/twhin-bert-base": 8,
"Twitter/twhin-bert-large": 4,
"xlm-roberta-large": 4,
"medicalai/ClinicalBERT": 64,
"dbmdz/bert-base-historic-multilingual-cased": 64,
"Davlan/afro-xlmr-large": 64,
"albert-xlarge-v2": 8,
"albert-xxlarge-v2": 4,
"bert-large-uncased-whole-word-masking": 64,
"Geotrend/bert-base-en-cased": 64,
"Geotrend/bert-base-10lang-cased": 64,
"Geotrend/bert-base-15lang-cased": 64,
"Intel/bert-base-uncased-sparse-70-unstructured": 64,
"Intel/bert-base-uncased-sparse-85-unstructured-pruneofa": 64,
"Intel/bert-base-uncased-sparse-90-unstructured-pruneofa": 64,
"Intel/distilbert-base-uncased-sparse-85-unstructured-pruneofa": 64,
"Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa": 64,
"abhi1nandy2/Bible-roberta-base": 64,
"cardiffnlp/twitter-roberta-base-irony": 64,
"distilbert-base-uncased-finetuned-sst-2-english": 64,
"roberta-large-openai-detector": 16,
"roberta-base-openai-detector": 32,
"tomh/toxigen_roberta": 32,
"gpt2": 8,
"distilgpt2": 8,
"openai-gpt": 8,
"gpt2-large": 2,
"xlnet-base-cased": 32,
"xlnet-large-cased": 16,
"distilbert-base-multilingual-cased": 32,
"google/electra-small-generator": 64,
"facebook/xlm-v-base": 8,
"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext": 32,
"microsoft/deberta-v3-base": 64,
"microsoft/deberta-v3-small": 64,
"microsoft/deberta-v3-large": 16,
"microsoft/deberta-v2-xlarge": 8,
"microsoft/mdeberta-v3-base": 64,
"microsoft/deberta-base": 64,
"distilbert-base-cased": 64,
"emilyalsentzer/Bio_ClinicalBERT": 64,
"yikuan8/Clinical-Longformer": 64,
"nlpaueb/legal-bert-base-uncased": 64,
"bert-base-cased": 64,
"EleutherAI/pythia-160m": 32,
"EleutherAI/pythia-410m": 8,
"EleutherAI/pythia-1b": 4,
"EleutherAI/pythia-1.4b": 4,
"meta-llama/Llama-3.2-1B": 4
}
