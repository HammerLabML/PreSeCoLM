{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39ce6d1-3ab0-4edf-9568-1472238c3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from data_loader import get_dataset\n",
    "from data_loader import BiosDataset, JigsawBias, CustomDataset, CrowSPairs, StereoSet, TwitterAAE, SBICDataset\n",
    "\n",
    "#import tiktoken\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0f052-d490-495b-a92a-45eafe2fe490",
   "metadata": {},
   "source": [
    "### (!) Specifiy your API Key\n",
    "\n",
    "Create a file `api_key.txt` with your OpenAI API Key to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128c368d-7b69-4d44-b23d-9e356b1d7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sail_api_key.txt', 'r') as file:\n",
    "    api_key = file.read().rstrip()\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6422a88b-5f29-44fd-bb60-a016ca51a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-3-large\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 800  # the maximum for text-embedding-3-small is 8191\n",
    "emb_dir = '../../artifacts/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791452c1-2403-4ad9-a4d4-232968924fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_embeddings(texts, save_file, embedding_model):\n",
    "    if os.path.exists(save_file):\n",
    "        with open(save_file, 'rb') as handle:\n",
    "            out_chunks = pickle.load(handle)\n",
    "    else:\n",
    "        out_chunks = []\n",
    "        for i in tqdm(range(0, len(texts), 10)):\n",
    "            out_chunks.append(client.embeddings.create(input=texts[i:i+10], model=embedding_model))\n",
    "\n",
    "    return out_chunks\n",
    "\n",
    "\n",
    "def get_embeddings(texts, save_file_raw, save_file_np, embedding_model):\n",
    "    if os.path.exists(save_file_np):\n",
    "        with open(save_file_np, 'rb') as handle:\n",
    "            emb_dict = pickle.load(handle)\n",
    "            assert emb_dict['model'] == embedding_model\n",
    "            embeddings = emb_dict['embeddings']\n",
    "            assert len(embeddings) == len(texts), (\"found %i embeddings for %i texts\" % (len(embeddings), len(texts)))\n",
    "    else:\n",
    "        out_chunks = get_raw_embeddings(texts, save_file_raw, embedding_model)\n",
    "\n",
    "        with open(save_file_raw, 'wb') as handle:\n",
    "            pickle.dump(out_chunks, handle)\n",
    "            \n",
    "        # remove chunks from raw embedding list while filling new list (necessary for larger datasets)\n",
    "        embeddings = []\n",
    "        while len(out_chunks) > 0:\n",
    "            chunk = out_chunks.pop(0)\n",
    "            for elem in chunk.data:\n",
    "                embeddings.append(elem.embedding)\n",
    "        \n",
    "        assert len(embeddings) == len(texts), (\"found %i embeddings for %i texts\" % (len(embeddings), len(texts)))\n",
    "\n",
    "        emb_arr = np.asarray(embeddings)\n",
    "        saved = {'model': embedding_model, 'embeddings': emb_arr}\n",
    "        \n",
    "        with open(save_file_np, 'wb') as handle:\n",
    "            pickle.dump(saved, handle)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fb2d3f-8bf1-4699-b968-694ca58b72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_emb(data, split, dataset_name, embedding_model):\n",
    "    print(\"got %i samples for split %s\" % (len(data), split))\n",
    "    save_file_raw = ('%s/%s_%s_%s_raw_output.pickle' % (emb_dir, dataset_name, split, embedding_model))\n",
    "    save_file_np = ('%s/%s_%s_%s.pickle' % (emb_dir, dataset_name, split, embedding_model))\n",
    "    embeddings = get_embeddings(data, save_file_raw, save_file_np, embedding_model)\n",
    "    \n",
    "    text_file = ('%s/%s_%s_text_data.pickle' % (emb_dir, dataset_name, split))\n",
    "    if not os.path.exists(text_file):\n",
    "        with open(text_file, 'wb') as handle:\n",
    "            pickle.dump(data, handle)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112ab98-a3fd-44a2-9421-43190e4866c2",
   "metadata": {},
   "source": [
    "### Embedd splits of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff64bef9-b8ea-48ff-8e88-e409891a5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load crowspairs\n",
      "got 3016 samples for split test\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'crows_pairs'\n",
    "dataset = CrowSPairs()\n",
    "data, _, lbl, group_lbl, cw, gw = dataset.get_split('test')\n",
    "\n",
    "emb = {}\n",
    "emb['test'] = get_split_emb(data, 'test', dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7c5f053-1df4-4621-85b1-553e63a0e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Stereoset with option both\n",
      "Load inter- and intrasentence samples and merge them to one dataset\n",
      "got 12687 samples for split val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1269/1269 [18:32<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'stereoset'\n",
    "dataset = StereoSet()\n",
    "data, _, lbl, group_lbl, cw, gw = dataset.get_split('val')\n",
    "\n",
    "emb = {}\n",
    "emb['val'] = get_split_emb(data, 'val', dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b91e70a9-8e28-4756-b668-4d4fab013591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load twitterAAE\n",
      "got 100000 samples for split test\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'twitterAAE'\n",
    "dataset = TwitterAAE()\n",
    "data, _, lbl, group_lbl, cw, gw = dataset.get_split('test')\n",
    "\n",
    "emb = {}\n",
    "emb['test'] = get_split_emb(data, 'test', dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b4665e-a458-4756-a6d7-48bbc57b7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load SBIC with local file: ../../data/filtered_sbic_minority_overview.csv\n",
      "compute class weights for split train\n",
      "compute class weights for split test\n",
      "compute class weights for split dev\n",
      "got 35933 samples for split train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3594/3594 [56:32<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 4705 samples for split test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 471/471 [06:43<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 4680 samples for split dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 468/468 [06:45<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sbic'\n",
    "local_dir = '../../data/filtered_sbic_minority_overview.csv'\n",
    "dataset = SBICDataset(local_dir=local_dir)\n",
    "\n",
    "emb = {}\n",
    "for split in ['train', 'test', 'dev']:\n",
    "    data, _, lbl, group_lbl, cw, gw = dataset.get_split(split)\n",
    "    emb[split] = get_split_emb(data, split, dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4814920-95f1-4b0b-95b9-ffda6afedea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load JigsawBias with option: single-class\n",
      "got 357019 samples for split train\n",
      "got 19042 samples for split test\n",
      "got 18876 samples for split dev\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'jigsaw'\n",
    "local_dir = '../../data/jigsaw_bias'\n",
    "dataset = JigsawBias(local_dir=local_dir, option='single-class')\n",
    "\n",
    "emb = {}\n",
    "for split in ['train', 'test', 'dev']:\n",
    "    data, _, lbl, group_lbl, cw, gw = dataset.get_split(split)\n",
    "    emb[split] = get_split_emb(data, split, dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "158dba66-d2c9-4ef4-991c-6e4564bbfde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load BIOS with option supervised\n",
      "got 7017 samples for split train\n",
      "got 2500 samples for split test\n",
      "got 1046 samples for split dev\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'bios-supervised'\n",
    "local_dir = '../../data/bios_huggingface_merge.pkl'\n",
    "dataset = BiosDataset(local_dir=local_dir, option='supervised')\n",
    "\n",
    "emb = {}\n",
    "for split in ['train', 'test', 'dev']:\n",
    "    data, _, lbl, group_lbl, cw, gw = dataset.get_split(split)\n",
    "    emb[split] = get_split_emb(data, split, dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62b9e610-cb41-475c-9253-807f38269c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load BIOS with option unsupervised\n",
      "got 257478 samples for split train\n",
      "got 99069 samples for split test\n",
      "got 39642 samples for split dev\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'bios-unsupervised'\n",
    "dataset = BiosDataset(local_dir=local_dir, option='unsupervised')\n",
    "\n",
    "emb = {}\n",
    "for split in ['train', 'test', 'dev']:\n",
    "    data, _, lbl, group_lbl, cw, gw = dataset.get_split(split)\n",
    "    emb[split] = get_split_emb(data, split, dataset_name, embedding_model)\n",
    "dataset.set_preprocessed_data(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98accde0-38f8-406c-96f1-fc11d10104af",
   "metadata": {},
   "source": [
    "### Embed training and test splits of the datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "575710fb-6e79-47d7-95b6-dd00871f6ab7",
   "metadata": {},
   "source": [
    "# bios supervised\n",
    "dataset_name = 'bios-supervised'\n",
    "bios_dir = '../../data/bios_huggingface_merge.pkl'\n",
    "X_train, y_train, X_test, y_test, n_classes, multi_label, class_weights, protected_attr_dict = get_dataset(dataset_name, local_dir=bios_dir)\n",
    "\n",
    "emb = {}\n",
    "emb['train'] = get_split_emb(X_train, 'train', dataset_name, embedding_model)\n",
    "emb['test'] = get_split_emb(X_test, 'test', dataset_name, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9699dc-82c3-426f-a9ca-1ef9f2cd8c1e",
   "metadata": {},
   "source": [
    "### Create dictionary with words/phrases used as defining terms in the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e271b53-1104-467e-abd3-b551ce6cb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_empty = '../../artifacts/embeddings/word_phrase_dict_empty.pickle'\n",
    "\n",
    "with open(dict_empty, 'rb') as handle:\n",
    "    word_phrase_emb_dict_empty = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0c631e2-1e25-4208-88e1-3c3ff938b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update word phrase dictionary given an experiment config\n",
    "CONFIG_FILE = 'experiments/configs/new/experiment_config.json'\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "eval_setup_file = config['bias_space_eval_config']\n",
    "with open(eval_setup_file, 'r') as stream:\n",
    "    eval_setups_by_attr = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1413236a-966b-46e0-a7d0-ac5e01544c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_phrase_emb_dict_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f677f524-ad08-4597-b9d1-4288eba0ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, content in eval_setups_by_attr.items():\n",
    "    assert len(content['defining_terms']) == 1\n",
    "\n",
    "    for attr, group_term_dict in content['defining_terms'].items():\n",
    "        for group, terms in group_term_dict.items():\n",
    "            for term in terms:\n",
    "                if not term in word_phrase_emb_dict_empty.keys():\n",
    "                    word_phrase_emb_dict_empty[term] = None\n",
    "\n",
    "with open(dict_empty, 'wb') as handle:\n",
    "     pickle.dump(word_phrase_emb_dict_empty, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8b2485f-2691-434d-a3e5-19c7a9606b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_phrase_emb_dict_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e53b014e-ee31-404d-98f8-3b66100facdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load or create new word/phrase dictionary for the current embedding model\n",
    "dict_emb = ('../../artifacts/embeddings/word_phrase_dict_%s.pickle' % embedding_model)\n",
    "\n",
    "# load current state of dictionary (if available)\n",
    "if os.path.exists(dict_emb):\n",
    "    with open(dict_emb, 'rb') as handle:\n",
    "        loaded_dict = pickle.load(handle)\n",
    "        prev_model = loaded_dict['model']\n",
    "        assert prev_model == embedding_model\n",
    "        word_phrase_emb_dict = loaded_dict['emb_dict']\n",
    "else:\n",
    "    word_phrase_emb_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0298d-c7b3-4aa4-a73e-b356a22e00ed",
   "metadata": {},
   "source": [
    "### Embed the terms and phrases from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea0c7ca7-6d98-4330-9865-d20614c84511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call api for middle eastern people\n",
      "call api for middle easterners\n",
      "call api for middle eastern american\n",
      "call api for a middle easterner\n",
      "call api for some middle easterner\n",
      "call api for the person is middle eastern\n",
      "call api for she is always working\n",
      "call api for he doesn't have any\n",
      "call api for i am about to go to the store\n",
      "call api for she is my sister\n",
      "call api for they were at the party\n",
      "call api for we have finished our homework\n",
      "call api for you are always joking around\n",
      "call api for i haven't seen him today\n",
      "call api for it doesn't matter\n",
      "call api for she got her hair done\n",
      "call api for she has known him for a long time\n",
      "call api for that's my brother's car\n",
      "call api for yesterday, he walked to the store\n",
      "call api for i don't know anything\n",
      "call api for he will be going to the store tomorrow\n",
      "call api for he told me the story\n",
      "call api for are you all coming to the party\n",
      "call api for she runs fast\n",
      "call api for there are a lot of people here\n",
      "call api for she be workin' all the time\n",
      "call api for he ain't got none\n",
      "call api for i'm finna go to the store\n",
      "call api for she my sister\n",
      "call api for they was at the party\n",
      "call api for we done finished our homework\n",
      "call api for you stay playin' too much\n",
      "call api for i ain't seen him today\n",
      "call api for it don't matter\n",
      "call api for she got her hair did\n",
      "call api for she been knowing him\n",
      "call api for that my brother car\n",
      "call api for yesterday, he walk to the store\n",
      "call api for i don't know nothing\n",
      "call api for he be going to the store tomorrow\n",
      "call api for he tol' me the story\n",
      "call api for y'all coming to the party\n",
      "call api for she run fast\n",
      "call api for it's a lot of people here\n"
     ]
    }
   ],
   "source": [
    "# query word/phrase embedding for current embedding model\n",
    "save_dict = {'model': embedding_model, 'emb_dict': word_phrase_emb_dict}\n",
    "\n",
    "for term, emb in word_phrase_emb_dict_empty.items():\n",
    "    if term in word_phrase_emb_dict.keys() and word_phrase_emb_dict[term] is not None:\n",
    "        # embedding for this term or phrase already exists\n",
    "        continue\n",
    "    else:\n",
    "        # call api\n",
    "        print(\"call api for %s\" % term)\n",
    "        emb = client.embeddings.create(input=[term], model=embedding_model).data[0].embedding\n",
    "        save_dict['emb_dict'][term] = emb\n",
    "\n",
    "with open(dict_emb, 'wb') as handle:\n",
    "    pickle.dump(save_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15342862-349e-4840-bc23-df4281e72a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_dict['emb_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b50d1-927b-4896-99fa-1371afe5745b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae6b07-90d6-4730-b1f3-f13f3c6da81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c4abd-6a32-48b9-b340-cf5a37a7d2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9ef16-e823-4750-8559-e4806021a8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
