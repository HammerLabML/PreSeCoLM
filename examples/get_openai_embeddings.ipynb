{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39ce6d1-3ab0-4edf-9568-1472238c3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pie_data import get_dataset\n",
    "#import tiktoken\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128c368d-7b69-4d44-b23d-9e356b1d7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt', 'r') as file:\n",
    "    api_key = file.read().rstrip()\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6422a88b-5f29-44fd-bb60-a016ca51a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-3-large\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 800  # the maximum for text-embedding-3-small is 8191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791452c1-2403-4ad9-a4d4-232968924fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_embeddings(texts, save_file, embedding_model):\n",
    "    if os.path.exists(save_file):\n",
    "        with open(save_file, 'rb') as handle:\n",
    "            out_chunks = pickle.load(handle)\n",
    "    else:\n",
    "        out_chunks = []\n",
    "        for i in tqdm(range(0, len(texts), 10)):\n",
    "            out_chunks.append(client.embeddings.create(input=texts[i:i+10], model=embedding_model))\n",
    "\n",
    "    return out_chunks\n",
    "\n",
    "\n",
    "def get_embeddings(texts, save_file_raw, save_file_np, embedding_model):\n",
    "    if os.path.exists(save_file_np):\n",
    "        with open(save_file_np, 'rb') as handle:\n",
    "            emb_dict = pickle.load(handle)\n",
    "            assert emb_dict['model'] == embedding_model\n",
    "            embeddings = emb_dict['embeddings']\n",
    "            assert len(embeddings) == len(texts), (\"found %i embeddings for %i texts\" % (len(embeddings), len(texts)))\n",
    "    else:\n",
    "        out_chunks = get_raw_embeddings(texts, save_file_raw, embedding_model)\n",
    "\n",
    "        with open(save_file_raw, 'wb') as handle:\n",
    "            pickle.dump(out_chunks, handle)\n",
    "            \n",
    "        # remove chunks from raw embedding list while filling new list (necessary for larger datasets)\n",
    "        embeddings = []\n",
    "        while len(out_chunks) > 0:\n",
    "            chunk = out_chunks.pop(0)\n",
    "            for elem in chunk.data:\n",
    "                embeddings.append(elem.embedding)\n",
    "        \n",
    "        assert len(embeddings) == len(texts), (\"found %i embeddings for %i texts\" % (len(embeddings), len(texts)))\n",
    "\n",
    "        emb_arr = np.asarray(embeddings)\n",
    "        saved = {'model': embedding_model, 'embeddings': emb_arr}\n",
    "        \n",
    "        with open(save_file_np, 'wb') as handle:\n",
    "            pickle.dump(saved, handle)\n",
    "        \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fb2d3f-8bf1-4699-b968-694ca58b72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_emb(data, split, dataset_name, embedding_model):\n",
    "    print(\"got %i samples for split %s\" % (len(data), split))\n",
    "    save_file_raw = ('embeddings/%s_%s_%s_raw_output.pickle' % (dataset_name, split, embedding_model))\n",
    "    save_file_np = ('embeddings/%s_%s_%s.pickle' % (dataset_name, split, embedding_model))\n",
    "    embeddings = get_embeddings(data, save_file_raw, save_file_np, embedding_model)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98accde0-38f8-406c-96f1-fc11d10104af",
   "metadata": {},
   "source": [
    "### Embed training and test splits of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d6f20a8-883d-4f00-aad4-094101484955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7017\n",
      "2500\n",
      "1046\n",
      "{'architect': 256.0, 'surgeon': 708.0, 'dentist': 117.0, 'teacher': 845.0, 'psychologist': 676.0, 'nurse': 432.0, 'photographer': 1195.0, 'physician': 824.0, 'attorney': 914.0, 'journalist': 1214.0}\n",
      "[26.41015625  8.91101695 58.97435897  7.30414201  9.38017751 15.24305556\n",
      "  4.87196653  7.5157767   6.67724289  4.7800659 ]\n",
      "{'architect': 118.0, 'surgeon': 359.0, 'dentist': 46.0, 'teacher': 454.0, 'psychologist': 313.0, 'nurse': 218.0, 'photographer': 599.0, 'physician': 379.0, 'attorney': 520.0, 'journalist': 632.0}\n",
      "[29.05084746  8.87743733 76.08695652  6.81057269 10.32907348 15.26605505\n",
      "  4.91986644  8.35620053  5.81923077  4.61075949]\n",
      "got 7017 samples for split train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 702/702 [16:54<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 3546 samples for split test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 355/355 [06:56<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# bios supervised\n",
    "dataset_name = 'bios-supervised'\n",
    "bios_dir = '../../data/bios_huggingface_merge.pkl'\n",
    "X_train, y_train, X_test, y_test, n_classes, multi_label, class_weights, protected_attr_dict = get_dataset(dataset_name, local_dir=bios_dir)\n",
    "\n",
    "emb = {}\n",
    "emb['train'] = get_split_emb(X_train, 'train', dataset_name, embedding_model)\n",
    "emb['test'] = get_split_emb(X_test, 'test', dataset_name, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f892a3-6e83-4470-b499-6540396b0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 100000 samples for split test\n"
     ]
    }
   ],
   "source": [
    "# twitterAAE\n",
    "dataset_name = 'twitterAAE'\n",
    "X_train, y_train, X_test, y_test, n_classes, multi_label, class_weights, protected_attr_dict = get_dataset(dataset_name)\n",
    "\n",
    "emb = {}\n",
    "emb['test'] = get_split_emb(X_test, 'test', dataset_name, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f4c455c-a04a-494d-8190-3c282e22d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 3016 samples for split test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 302/302 [06:31<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# crowspairs\n",
    "dataset_name = 'crows_pairs'\n",
    "X_train, y_train, X_test, y_test, n_classes, multi_label, class_weights, protected_attr_dict = get_dataset(dataset_name)\n",
    "\n",
    "emb = {}\n",
    "emb['test'] = get_split_emb(X_test, 'test', dataset_name, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04de773-f79a-467c-a917-026ea260ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bios unsupervised\n",
    "dataset_name = 'bios-unsupervised'\n",
    "X_train, y_train, X_test, y_test, n_classes, multi_label, class_weights, protected_attr_dict = get_dataset(dataset_name)\n",
    "\n",
    "emb = {}\n",
    "emb['train'] = get_split_emb(X_train, 'train', dataset_name, embedding_model)\n",
    "emb['test'] = get_split_emb(X_test, 'test', dataset_name, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86086f7f-07d1-4e5a-a1ec-6b2a2a2a2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 18505.0}\n",
      "[18.29310997]\n",
      "{'target': 971.0}\n",
      "[18.43975283]\n"
     ]
    }
   ],
   "source": [
    "# jigsaw\n",
    "dataset_name = 'jigsaw'\n",
    "local_dir = '../../data/jigsaw_bias'\n",
    "X_train, y_train, X_test, y_test, n_classes, multi_label, class_weights, protected_attr_dict = get_dataset(dataset_name, local_dir)\n",
    "\n",
    "emb = {}\n",
    "emb['train'] = get_split_emb(X_train, 'train', dataset_name, embedding_model)\n",
    "emb['test'] = get_split_emb(X_test, 'test', dataset_name, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9699dc-82c3-426f-a9ca-1ef9f2cd8c1e",
   "metadata": {},
   "source": [
    "### Create dictionary with words/phrases used as defining terms in the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e271b53-1104-467e-abd3-b551ce6cb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_empty = 'embeddings/word_phrase_dict_empty.pickle'\n",
    "\n",
    "with open(dict_empty, 'rb') as handle:\n",
    "    word_phrase_emb_dict_empty = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c631e2-1e25-4208-88e1-3c3ff938b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update word phrase dictionary given an experiment config\n",
    "CONFIG_FILE = 'experiments/configs/esann25/experiment_config.json'\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "eval_setup_file = config['bias_space_eval_config']\n",
    "with open(eval_setup_file, 'r') as stream:\n",
    "    eval_setups_by_attr = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f677f524-ad08-4597-b9d1-4288eba0ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, content in eval_setups_by_attr.items():\n",
    "    assert len(content['defining_terms']) == 1\n",
    "\n",
    "    for group_terms in content['defining_terms'][0]:\n",
    "        for term in group_terms:\n",
    "            if not term in word_phrase_emb_dict_empty.keys():\n",
    "                word_phrase_emb_dict_empty[term] = None\n",
    "\n",
    "with open(dict_empty, 'wb') as handle:\n",
    "     pickle.dump(word_phrase_emb_dict_empty, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e53b014e-ee31-404d-98f8-3b66100facdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load or create new word/phrase dictionary for the current embedding model\n",
    "dict_emb = ('embeddings/word_phrase_dict_%s.pickle' % embedding_model)\n",
    "\n",
    "# load current state of dictionary (if available)\n",
    "if os.path.exists(dict_emb):\n",
    "    with open(dict_emb, 'rb') as handle:\n",
    "        loaded_dict = pickle.load(handle)\n",
    "        prev_model = loaded_dict['model']\n",
    "        assert prev_model == embedding_model\n",
    "        word_phrase_emb_dict = loaded_dict['emb_dict']\n",
    "else:\n",
    "    word_phrase_emb_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0298d-c7b3-4aa4-a73e-b356a22e00ed",
   "metadata": {},
   "source": [
    "### Embed the terms and phrases from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea0c7ca7-6d98-4330-9865-d20614c84511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call api for mentally fit\n",
      "call api for sane\n",
      "call api for of sound mind\n",
      "call api for healthy mind\n",
      "call api for clearheaded\n",
      "call api for has eating habits\n",
      "call api for is slightly worried\n",
      "call api for has bad memories\n",
      "call api for normal\n",
      "call api for mentally ill\n",
      "call api for demented\n",
      "call api for insane\n",
      "call api for addict\n",
      "call api for depression\n",
      "call api for schizophrenia\n",
      "call api for bipolar\n",
      "call api for has eating disorder\n",
      "call api for has anxiety\n",
      "call api for has ptsd\n",
      "call api for retarded\n",
      "call api for caucasian\n",
      "call api for african\n",
      "call api for asian\n",
      "call api for latino\n"
     ]
    }
   ],
   "source": [
    "# query word/phrase embedding for current embedding model\n",
    "save_dict = {'model': embedding_model, 'emb_dict': word_phrase_emb_dict}\n",
    "\n",
    "for term, emb in word_phrase_emb_dict_empty.items():\n",
    "    if term in word_phrase_emb_dict.keys() and word_phrase_emb_dict[term] is not None:\n",
    "        # embedding for this term or phrase already exists\n",
    "        continue\n",
    "    else:\n",
    "        # call api\n",
    "        print(\"call api for %s\" % term)\n",
    "        emb = client.embeddings.create(input=[term], model=embedding_model).data[0].embedding\n",
    "        save_dict['emb_dict'][term] = emb\n",
    "\n",
    "with open(dict_emb, 'wb') as handle:\n",
    "    pickle.dump(save_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15342862-349e-4840-bc23-df4281e72a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_dict['emb_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b50d1-927b-4896-99fa-1371afe5745b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae6b07-90d6-4730-b1f3-f13f3c6da81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
